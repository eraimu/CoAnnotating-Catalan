{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e321e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9850912",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a73e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access Catalan dataset\n",
    "splits = {'train': 'catalan/train-00000-of-00001.parquet', 'test': 'catalan/test-00000-of-00001.parquet', 'validation': 'catalan/validation-00000-of-00001.parquet'}\n",
    "\n",
    "df_train = pd.read_parquet(\"hf://datasets/mteb/CataloniaTweetClassification/\" + splits[\"train\"])\n",
    "df_test = pd.read_parquet(\"hf://datasets/mteb/CataloniaTweetClassification/\" + splits[\"test\"])\n",
    "df_val = pd.read_parquet(\"hf://datasets/mteb/CataloniaTweetClassification/\" + splits[\"validation\"])\n",
    "\n",
    "# save locally\n",
    "df_train.to_csv(\"train_cat.csv\", index=False)\n",
    "df_test.to_csv(\"test_cat.csv\", index=False)\n",
    "df_val.to_csv(\"val_cat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access Spanish dataset\n",
    "splits = {'train': 'spanish/train-00000-of-00001.parquet', 'test': 'spanish/test-00000-of-00001.parquet', 'validation': 'spanish/validation-00000-of-00001.parquet'}\n",
    "\n",
    "df_train = pd.read_parquet(\"hf://datasets/mteb/CataloniaTweetClassification/\" + splits[\"train\"])\n",
    "df_test = pd.read_parquet(\"hf://datasets/mteb/CataloniaTweetClassification/\" + splits[\"test\"])\n",
    "df_val = pd.read_parquet(\"hf://datasets/mteb/CataloniaTweetClassification/\" + splits[\"validation\"])\n",
    "\n",
    "# save locally\n",
    "df_train.to_csv(\"train_es.csv\", index=False)\n",
    "df_test.to_csv(\"test_es.csv\", index=False)\n",
    "df_val.to_csv(\"val_es.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfcc1c",
   "metadata": {},
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset as df\n",
    "df = pd.read_csv(path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first few rows\n",
    "display(df.head())\n",
    "\n",
    "# dataset shape\n",
    "print(\"\\n Dataset shape:\", df.shape)\n",
    "\n",
    "# column names\n",
    "print(\"\\n Columns:\", df.columns.tolist())\n",
    "\n",
    "# label distribution \n",
    "print(\"\\n Label distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\n Label distribution (%)\")\n",
    "print(df['label'].value_counts(normalize=True).round(3) * 100)\n",
    "\n",
    "# average tweet length (in characters and words)\n",
    "df['char_len'] = df['text'].str.len()\n",
    "df['word_len'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"\\n Average tweet length (chars):\", df['char_len'].mean())\n",
    "print(\" Average tweet length (words):\", df['word_len'].mean())\n",
    "\n",
    "# check for duplicates and missing values\n",
    "print(\"\\n Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n Number of duplicate tweets:\", df.duplicated(subset='text').sum())\n",
    "\n",
    "# find duplicates\n",
    "duplicates = df[df.duplicated(subset='text', keep=False)]\n",
    "\n",
    "print(f\" Found {len(duplicates)} duplicate rows\")\n",
    "\n",
    "# print duplicates\n",
    "duplicates = duplicates.sort_values(by=\"text\")\n",
    "display(duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09df3a9",
   "metadata": {},
   "source": [
    "### Duplicate removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e403635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates from Spanish dataset\n",
    "df_es = pd.read_csv(path, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Before removing duplicates:\", df_es.shape)\n",
    "\n",
    "# dropping all duplicated tweets \n",
    "df_es = df_es[df_es.duplicated(subset=\"text\", keep=False) == False].reset_index(drop=True)\n",
    "\n",
    "print(\"After removing duplicates:\", df_es.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd69ccf",
   "metadata": {},
   "source": [
    "### Portuguese tweets removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to drop portuguese tweets\n",
    "def drop_portuguese(df, text_col=\"text\"):\n",
    "    langs = []\n",
    "    for t in df[text_col]:\n",
    "        try:\n",
    "            langs.append(detect(t))\n",
    "        except:\n",
    "            langs.append(\"error\")\n",
    "    \n",
    "    df[\"lang_detected\"] = langs\n",
    "    df_filtered = df[df[\"lang_detected\"] != \"pt\"].reset_index(drop=True)\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing portuguese tweets \n",
    "df_es_clean = drop_portuguese(df_es, text_col=\"text\")\n",
    "print(\"After removing portuguese tweets:\", df_es_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7125c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check new label distribution \n",
    "print(\"\\n Label distribution:\")\n",
    "print(df_es_clean['label'].value_counts())\n",
    "print(\"\\n Label distribution (%)\")\n",
    "print(df_es_clean['label'].value_counts(normalize=True).round(3) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2622c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_es_clean.to_csv(\"train_es_clean.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc1708",
   "metadata": {},
   "source": [
    "### Subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99134fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 nested stratified chunks from the cleaned dataset\n",
    "\n",
    "def stratified_chunks(csv_path, output_prefix, total_size=2000, chunk_size=100, random_state=42):\n",
    "        \n",
    "    # load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(\"Full dataset size:\", len(df))\n",
    "    print(\"Label distribution in full dataset (%):\")\n",
    "    print(df['label'].value_counts(normalize=True).round(3) * 100, \"\\n\")\n",
    "    \n",
    "    # stratified sample of total_size\n",
    "    df_sampled, _ = train_test_split(\n",
    "        df,\n",
    "        train_size=total_size,\n",
    "        stratify=df['label'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # create 5 nested stratified chunks\n",
    "    chunks = []\n",
    "    remaining = df_sampled.copy()\n",
    "    \n",
    "    for i in range(0, total_size, chunk_size):\n",
    "        chunk, remaining = train_test_split(\n",
    "            remaining,\n",
    "            train_size=chunk_size,\n",
    "            stratify=remaining['label'],\n",
    "            random_state=random_state + i  \n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # save individual chunk\n",
    "        chunk.to_csv(f\"{output_prefix}_chunk{i//chunk_size + 1}.csv\", index=False)\n",
    "    \n",
    "    # save cumulative subsets\n",
    "    for i in range(len(chunks)):\n",
    "        cumulative = pd.concat(chunks[:i+1])\n",
    "        cumulative.to_csv(f\"{output_prefix}_{(i+1)*chunk_size}.csv\", index=False)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d844c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Catalan subset selection\n",
    "stratified_chunks(r\"C:\\Users\\emmar\\Documents\\GitHub\\coannotating-catalan\\data\\CAT_dataset\\train_cat.csv\", output_prefix=\"subset_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca84efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish subsets selection\n",
    "stratified_chunks(r\"C:\\Users\\emmar\\Documents\\GitHub\\coannotating-catalan\\data\\ES_dataset\\train_es_clean.csv\", output_prefix=\"subset_es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1c895",
   "metadata": {},
   "source": [
    "### Chunk cleaning + indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4995115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ead6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove line breaks and tabs and normalize multiple spaces\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf191a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to unique IDs across multiple chunk CSVs for a given language\n",
    "def clean_and_ids(chunk_path_pattern, lang_prefix, save_overwrite=False):\n",
    "        \n",
    "    chunk_files = sorted(glob.glob(chunk_path_pattern))\n",
    "    if not chunk_files:\n",
    "        print(\"⚠️ No files found with that pattern.\")\n",
    "        return []\n",
    "    \n",
    "    current_id = 0\n",
    "    all_chunks = []\n",
    "    \n",
    "    for file in chunk_files:\n",
    "        df = pd.read_csv(file)\n",
    "        n_rows = len(df)\n",
    "        \n",
    "        # unique IDs\n",
    "        df[\"id\"] = [f\"{lang_prefix}_{str(i).zfill(3)}\" for i in range(current_id, current_id + n_rows)]\n",
    "        current_id += n_rows\n",
    "                \n",
    "        # clean text column\n",
    "        df[\"text\"] = df[\"text\"].apply(lambda text: \" \".join(text.replace(\"\\n\", \" \")\n",
    "                                                           .replace(\"\\r\", \" \")\n",
    "                                                           .replace(\"\\t\", \" \")\n",
    "                                                           .split()))\n",
    "        # reorder columns\n",
    "        df = df[[\"id\", \"text\", \"label\"]]\n",
    "        \n",
    "        all_chunks.append(df)\n",
    "        \n",
    "        # save file\n",
    "        if save_overwrite:\n",
    "            df.to_csv(file, index=False)\n",
    "        else:\n",
    "            base, ext = os.path.splitext(file)\n",
    "            df.to_csv(f\"{base}_final.csv\", index=False)\n",
    "       \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78a79a",
   "metadata": {},
   "source": [
    "#### Catalan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edced654",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_ids(r\"C:\\Users\\emmar\\Documents\\GitHub\\coannotating-catalan\\data\\CAT_dataset\\subset_cat_chunk*.csv\", \"CAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ae621",
   "metadata": {},
   "source": [
    "#### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_ids(r\"C:\\Users\\emmar\\Documents\\GitHub\\coannotating-catalan\\data\\ES_dataset\\subset_es_chunk*.csv\", \"ES\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
